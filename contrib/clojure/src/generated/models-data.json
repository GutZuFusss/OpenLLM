{
  "chatglm": {
    "architecture": "ChatGLMForConditionalGeneration",
    "model_id": [
      "thudm/chatglm-6b",
      "thudm/chatglm-6b-int8",
      "thudm/chatglm-6b-int4",
      "thudm/chatglm2-6b",
      "thudm/chatglm2-6b-int4"
    ],
    "cpu": false,
    "gpu": true,
    "runtime_impl": [
      "pt"
    ],
    "installation": "\"openllm[chatglm]\""
  },
  "dolly-v2": {
    "architecture": "GPTNeoXForCausalLM",
    "model_id": [
      "databricks/dolly-v2-3b",
      "databricks/dolly-v2-7b",
      "databricks/dolly-v2-12b"
    ],
    "cpu": true,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "vllm"
    ],
    "installation": "openllm"
  },
  "falcon": {
    "architecture": "FalconForCausalLM",
    "model_id": [
      "tiiuae/falcon-7b",
      "tiiuae/falcon-40b",
      "tiiuae/falcon-7b-instruct",
      "tiiuae/falcon-40b-instruct"
    ],
    "cpu": false,
    "gpu": true,
    "runtime_impl": [
      "pt"
    ],
    "installation": "\"openllm[falcon]\""
  },
  "flan-t5": {
    "architecture": "T5ForConditionalGeneration",
    "model_id": [
      "google/flan-t5-small",
      "google/flan-t5-base",
      "google/flan-t5-large",
      "google/flan-t5-xl",
      "google/flan-t5-xxl"
    ],
    "cpu": true,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "flax",
      "tf"
    ],
    "installation": "\"openllm[flan-t5]\""
  },
  "gpt-neox": {
    "architecture": "GPTNeoXForCausalLM",
    "model_id": [
      "eleutherai/gpt-neox-20b"
    ],
    "cpu": false,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "vllm"
    ],
    "installation": "openllm"
  },
  "llama": {
    "architecture": "LlamaForCausalLM",
    "model_id": [
      "meta-llama/Llama-2-70b-chat-hf",
      "meta-llama/Llama-2-13b-chat-hf",
      "meta-llama/Llama-2-7b-chat-hf",
      "meta-llama/Llama-2-70b-hf",
      "meta-llama/Llama-2-13b-hf",
      "meta-llama/Llama-2-7b-hf",
      "NousResearch/llama-2-70b-chat-hf",
      "NousResearch/llama-2-13b-chat-hf",
      "NousResearch/llama-2-7b-chat-hf",
      "NousResearch/llama-2-70b-hf",
      "NousResearch/llama-2-13b-hf",
      "NousResearch/llama-2-7b-hf",
      "openlm-research/open_llama_7b_v2",
      "openlm-research/open_llama_3b_v2",
      "openlm-research/open_llama_13b",
      "huggyllama/llama-65b",
      "huggyllama/llama-30b",
      "huggyllama/llama-13b",
      "huggyllama/llama-7b"
    ],
    "cpu": true,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "vllm"
    ],
    "installation": "\"openllm[llama]\""
  },
  "mpt": {
    "architecture": "MPTForCausalLM",
    "model_id": [
      "mosaicml/mpt-7b",
      "mosaicml/mpt-7b-instruct",
      "mosaicml/mpt-7b-chat",
      "mosaicml/mpt-7b-storywriter",
      "mosaicml/mpt-30b",
      "mosaicml/mpt-30b-instruct",
      "mosaicml/mpt-30b-chat"
    ],
    "cpu": true,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "vllm"
    ],
    "installation": "\"openllm[mpt]\""
  },
  "opt": {
    "architecture": "OPTForCausalLM",
    "model_id": [
      "facebook/opt-125m",
      "facebook/opt-350m",
      "facebook/opt-1.3b",
      "facebook/opt-2.7b",
      "facebook/opt-6.7b",
      "facebook/opt-66b"
    ],
    "cpu": true,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "flax",
      "tf",
      "vllm"
    ],
    "installation": "\"openllm[opt]\""
  },
  "stablelm": {
    "architecture": "GPTNeoXForCausalLM",
    "model_id": [
      "stabilityai/stablelm-tuned-alpha-3b",
      "stabilityai/stablelm-tuned-alpha-7b",
      "stabilityai/stablelm-base-alpha-3b",
      "stabilityai/stablelm-base-alpha-7b"
    ],
    "cpu": true,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "vllm"
    ],
    "installation": "openllm"
  },
  "starcoder": {
    "architecture": "GPTBigCodeForCausalLM",
    "model_id": [
      "bigcode/starcoder",
      "bigcode/starcoderbase"
    ],
    "cpu": false,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "vllm"
    ],
    "installation": "\"openllm[starcoder]\""
  },
  "baichuan": {
    "architecture": "BaiChuanForCausalLM",
    "model_id": [
      "baichuan-inc/baichuan-7b",
      "baichuan-inc/baichuan-13b-base",
      "baichuan-inc/baichuan-13b-chat",
      "fireballoon/baichuan-vicuna-chinese-7b",
      "fireballoon/baichuan-vicuna-7b",
      "hiyouga/baichuan-7b-sft"
    ],
    "cpu": false,
    "gpu": true,
    "runtime_impl": [
      "pt",
      "vllm"
    ],
    "installation": "\"openllm[baichuan]\""
  }
}
